{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Caching with Gemini\n",
    "\n",
    "This notebook demonstrates how to use the context caching feature of the Gemini. Context caching allows you to store large amounts of context (like documents or lengthy instructions) with the model once, and then refer to that cached context in subsequent requests saving up to 75% cost. When you cache a set of tokens, you can choose how long you want the cache to exist before the tokens are automatically deleted. This caching duration is called the time to live (TTL). In the example we are going to cache the a github repository to then more easily ask questions on it. \n",
    "\n",
    "How it works:\n",
    "1. You create a cache inckuding the content (text, files), an optional system instruction, and a time-to-live (TTL).\n",
    "2. When generating content, you reference the cache name instead of adding the context to the `contents`. The model uses the cached information alongside your new prompt.\n",
    "\n",
    "Pricing\n",
    "e provided pricing (Paid Tier, per 1 Million Tokens, USD):\n",
    "\n",
    "| Model Name              | Input Cost (No Cache)   | Cached Input Cost        | Approx. Reduction |\n",
    "| :---------------------- | :---------------------- | :----------------------- | :---------------- |\n",
    "| **Gemini 2.5 Pro Preview** | $1.25 (≤200k tokens) <br/> $2.50 (>200k tokens)   | $0.31 (≤200k tokens) <br/>  $0.625 (>200k tokens) | ~75%              |\n",
    "| **Gemini 2.5 Flash Preview** | $0.15 (text / image / video)  | Coming soon)   | -            |\n",
    "| **Gemini 2.0 Flash**    | $0.10 (text/img/vid) <br/> $0.70 (audio)  | $0.025 (text/img/vid) <br/>  $0.175 (audio) | 75%               |\n",
    "\n",
    "\n",
    "Explore the documentation to implement these features with **Gemini 2.5 Pro**:\n",
    "*   Gemini API Caching Documentation: https://ai.google.dev/gemini-api/docs/caching?lang=python \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-genai gitingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from gitingest import ingest_async\n",
    "\n",
    "# create client\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\",\"xxx\"))\n",
    "\n",
    "model_id = \"gemini-2.5-pro-preview-03-25\" # \"gemini-2.0-flash\"\n",
    "system_instruction = \"You are a helpful coding assistant with the FastMCP github repository available in context. If a users asks question about FastMCP or how to build an MCP server use the available information.\"\n",
    "\n",
    "# Load the Fast MCP repository and exclude tests and bloated pattern\n",
    "summary, tree, content = await ingest_async(\"https://github.com/jlowin/fastmcp\",exclude_patterns=\"*.json, *.css, *.js, uv.lock, python-version, tests/, .github/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache Details:\n",
      "name: cachedContents/2gpwrkr99yew\n",
      "model: models/gemini-2.5-pro-preview-03-25\n",
      "expire_time: 2025-04-30T10:02:29+02:00\n",
      "Token Count: 153603 tokens\n"
     ]
    }
   ],
   "source": [
    "# Create a cached content object\n",
    "cache = client.caches.create(\n",
    "    model=model_id,\n",
    "    config=genai.types.CreateCachedContentConfig(\n",
    "      system_instruction=system_instruction,\n",
    "      contents=[content],\n",
    "      ttl=\"300s\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display the cache details\n",
    "print(f\"Cache Details:\\nname: {cache.name}\\nmodel: {cache.model}\\nexpire_time: {cache.expire_time.astimezone().isoformat(timespec='seconds')}\\nToken Count: {cache.usage_metadata.total_token_count} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached Tokens used: 153603\n",
      "No Cache Tokens used: 18\n",
      "Thoughts Tokens used: 1448\n",
      "Output Tokens used: 1676\n"
     ]
    }
   ],
   "source": [
    "# Generate content using the cached prompt and document\n",
    "response = client.models.generate_content(\n",
    "  model=model_id,\n",
    "  contents=\"Build a simple MCP server for reading and writing local files under /tmp/mcp\",\n",
    "  config=genai.types.GenerateContentConfig(\n",
    "    cached_content=cache.name\n",
    "  ))\n",
    "\n",
    "# Print usage metadata for insights into the API call\n",
    "print(f\"Cached Tokens used: {response.usage_metadata.cached_content_token_count}\\nNo Cache Tokens used: {response.usage_metadata.prompt_token_count - response.usage_metadata.cached_content_token_count}\\nThoughts Tokens used: {response.usage_metadata.thoughts_token_count}\\nOutput Tokens used: {response.usage_metadata.candidates_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Okay, here is a simple FastMCP server that allows reading, writing, and listing files within a specific local directory (`/tmp/mcp`).\n",
      "\n",
      "**Security Note:** This example restricts file operations to the `/tmp/mcp` directory. Allowing an LLM to access arbitrary local files is a significant security risk. Ensure you understand the implications before deploying such a tool in a real environment.\n",
      "\n",
      "```python\n",
      "# file_server.py\n",
      "import os\n",
      "from pathlib import Path\n",
      "\n",
      "from fastmcp import FastMCP, Context\n",
      "\n",
      "# --- Configuration ---\n",
      "# Define the base directory where files will be stored/accessed.\n",
      "# RESOLVE the path to prevent relative path issues.\n",
      "BASE_DIR = Path(\"/tmp/mcp\").resolve()\n",
      "\n",
      "# --- FastMCP Server Setup ---\n",
      "mcp = FastMCP(name=\"Local File Server\")\n",
      "\n",
      "# --- Security Helper Function ---\n",
      "def _resolve_safe_path(filename: str) -> Path:\n",
      "    \"\"\"\n",
      "    Resolves the filename relative to BASE_DIR and ensures it stays within BASE_DIR.\n",
      "\n",
      "    Args:\n",
      "        filename: The relative filename provided by the user.\n",
      "\n",
      "    Returns:\n",
      "        The resolved Path object.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the resolved path attempts to escape the BASE_DIR.\n",
      "        FileNotFoundError: If the file doesn't exist (for read operations specifically,\n",
      "                           though this check might be done in the calling tool).\n",
      "    \"\"\"\n",
      "    # Prevent absolute paths or directory traversal attempts in the input filename\n",
      "    if filename.startswith(\"/\") or \"..\" in filename:\n",
      "        raise ValueError(\"Invalid filename: Must be relative and within the allowed directory.\")\n",
      "\n",
      "    # Join with base directory and resolve to get the canonical path\n",
      "    full_path = (BASE_DIR / filename).resolve()\n",
      "\n",
      "    # MAJOR SECURITY CHECK: Ensure the resolved path is still inside the BASE_DIR\n",
      "    if BASE_DIR not in full_path.parents and full_path != BASE_DIR:\n",
      "         # The above check handles the case where full_path is exactly BASE_DIR\n",
      "         # and prevents accessing files directly within BASE_DIR's parent.\n",
      "        raise ValueError(\"Access denied: Path is outside the allowed directory.\")\n",
      "\n",
      "    return full_path\n",
      "\n",
      "# --- Tools ---\n",
      "\n",
      "@mcp.tool()\n",
      "def list_files(ctx: Context) -> list[str]:\n",
      "    \"\"\"Lists all files within the designated '/tmp/mcp' directory.\"\"\"\n",
      "    ctx.info(f\"Listing files in {BASE_DIR}\")\n",
      "    if not BASE_DIR.exists():\n",
      "        return [] # Return empty list if base directory doesn't exist\n",
      "\n",
      "    try:\n",
      "        # List only files, not directories, relative to BASE_DIR\n",
      "        files = [f.name for f in BASE_DIR.iterdir() if f.is_file()]\n",
      "        return files\n",
      "    except Exception as e:\n",
      "        ctx.error(f\"Error listing files: {e}\")\n",
      "        raise ValueError(f\"Could not list files: {e}\")\n",
      "\n",
      "@mcp.tool()\n",
      "def read_file(filename: str, ctx: Context) -> str:\n",
      "    \"\"\"\n",
      "    Reads the content of a specified file within the '/tmp/mcp' directory.\n",
      "\n",
      "    Args:\n",
      "        filename: The name of the file to read (relative to /tmp/mcp).\n",
      "        ctx: The MCP Context object.\n",
      "\n",
      "    Returns:\n",
      "        The content of the file as a string.\n",
      "    \"\"\"\n",
      "    ctx.info(f\"Attempting to read file: {filename}\")\n",
      "    try:\n",
      "        safe_path = _resolve_safe_path(filename)\n",
      "        ctx.debug(f\"Reading from safe path: {safe_path}\")\n",
      "\n",
      "        if not safe_path.is_file():\n",
      "             raise FileNotFoundError(f\"File '{filename}' not found.\")\n",
      "\n",
      "        content = safe_path.read_text()\n",
      "        ctx.info(f\"Successfully read file: {filename}\")\n",
      "        return content\n",
      "    except (ValueError, FileNotFoundError) as e:\n",
      "         ctx.warning(f\"Read operation failed for {filename}: {e}\")\n",
      "         raise # Re-raise the specific error for MCP to handle\n",
      "    except Exception as e:\n",
      "        ctx.error(f\"Unexpected error reading file {filename}: {e}\")\n",
      "        raise ValueError(f\"An unexpected error occurred while reading the file: {e}\")\n",
      "\n",
      "@mcp.tool()\n",
      "def write_file(filename: str, content: str, ctx: Context) -> str:\n",
      "    \"\"\"\n",
      "    Writes the given content to a specified file within the '/tmp/mcp' directory.\n",
      "    Creates the directory if it doesn't exist. Overwrites the file if it exists.\n",
      "\n",
      "    Args:\n",
      "        filename: The name of the file to write (relative to /tmp/mcp).\n",
      "        content: The string content to write to the file.\n",
      "        ctx: The MCP Context object.\n",
      "\n",
      "    Returns:\n",
      "        A confirmation message string.\n",
      "    \"\"\"\n",
      "    ctx.info(f\"Attempting to write to file: {filename}\")\n",
      "    try:\n",
      "        # Ensure the base directory exists\n",
      "        BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "        safe_path = _resolve_safe_path(filename)\n",
      "        ctx.debug(f\"Writing to safe path: {safe_path}\")\n",
      "\n",
      "        safe_path.write_text(content)\n",
      "        ctx.info(f\"Successfully wrote to file: {filename}\")\n",
      "        return f\"Successfully wrote content to '{filename}'.\"\n",
      "    except ValueError as e:\n",
      "        ctx.warning(f\"Write operation denied for {filename}: {e}\")\n",
      "        raise # Re-raise the specific error (likely path validation)\n",
      "    except Exception as e:\n",
      "        ctx.error(f\"Unexpected error writing file {filename}: {e}\")\n",
      "        raise ValueError(f\"An unexpected error occurred while writing the file: {e}\")\n",
      "\n",
      "\n",
      "# --- Main Execution ---\n",
      "if __name__ == \"__main__\":\n",
      "    # Ensure the base directory exists on startup\n",
      "    try:\n",
      "        BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
      "        print(f\"Serving files from directory: {BASE_DIR}\")\n",
      "    except OSError as e:\n",
      "        print(f\"Error creating base directory {BASE_DIR}: {e}\", file=sys.stderr)\n",
      "        # Decide if you want to exit or continue without the directory assured\n",
      "        # exit(1) # Uncomment to exit if directory creation fails\n",
      "\n",
      "    mcp.run()\n",
      "```\n",
      "\n",
      "**How to Run:**\n",
      "\n",
      "1.  Save the code above as `file_server.py`.\n",
      "2.  Create the directory: `mkdir -p /tmp/mcp` (or let the script create it on the first write).\n",
      "3.  Run the server from your terminal:\n",
      "    ```bash\n",
      "    python file_server.py\n",
      "    ```\n",
      "    Or using the FastMCP CLI (recommended for development/testing):\n",
      "    ```bash\n",
      "    fastmcp dev file_server.py\n",
      "    ```\n",
      "    Or install it for Claude Desktop:\n",
      "    ```bash\n",
      "    fastmcp install file_server.py --name \"Local TMP Files\"\n",
      "    ```\n",
      "\n",
      "**Interaction Example (in Claude):**\n",
      "\n",
      "*   \"List the files available.\" -> Calls `list_files`\n",
      "*   \"Write 'Hello MCP!' to a file named `greeting.txt`.\" -> Calls `write_file`\n",
      "*   \"Read the content of `greeting.txt`.\" -> Calls `read_file`\n",
      "*   \"Write 'test' to `../outside.txt`\" -> Should fail due to the security check in `_resolve_safe_path`.\n",
      "\n",
      "This example provides basic file operations while incorporating essential security checks to confine actions within the specified directory.\n"
     ]
    }
   ],
   "source": [
    "# Print the generated text\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit Caching\n",
    "\n",
    "The Gemini API supports [implicit caching](https://ai.google.dev/gemini-api/docs/caching?lang=python), unlocking automatic 75% cost savings when your requests hit the cache! This means if you send a request to Gemini 2.5 models with a common prefix as one of previous requests, it’s eligible for a cache hit. The minimum input token count for context caching is 1,024 for 2.5 Flash and 2,048 for 2.5 Pro.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 2581\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "# create client\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\",\"xxx\"))\n",
    "model_id = \"gemini-2.5-pro-preview-05-06\" # \"gemini-2.5-flash-preview-04-17\"\n",
    "\n",
    "# upload big pdf file\n",
    "file_path = \"../assets/2025q1-alphabet-earnings-release.pdf\"\n",
    "pdf_file = client.files.upload(file=file_path)\n",
    "\n",
    "\n",
    "# count tokens\n",
    "tokens = client.models.count_tokens(model=model_id, contents=[pdf_file])\n",
    "print(f\"Tokens: {tokens.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: 2596\n",
      "cached tokens: None\n",
      "output tokens: 3072\n"
     ]
    }
   ],
   "source": [
    "# Ask the model to summarize the earnings release\n",
    "instruction = \"Summarize the earnings release for the first quarter of 2025\"\n",
    "response_1 = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=[pdf_file, instruction],\n",
    ")\n",
    "\n",
    "# Print usage metadata for insights into the API call\n",
    "print(f\"input tokens: {response_1.usage_metadata.prompt_token_count }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached tokens: 2160\n"
     ]
    }
   ],
   "source": [
    "# 2nd request which uses the cached prefix (pdf file)\n",
    "instruction = \"What are focus areas for the second quarter of 2025?\"\n",
    "response_2 = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=[pdf_file, instruction],\n",
    ")\n",
    "\n",
    "print(f\"cached tokens: {response_2.usage_metadata.cached_content_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "# create client\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\",\"xxx\"))\n",
    "model_id = \"gemini-2.5-pro-preview-05-06\" # \"gemini-2.5-flash-preview-04-17\"\n",
    "\n",
    "\n",
    "cache_count = 0\n",
    "for i in range(10):\n",
    "    # upload big pdf file\n",
    "    file_path = \"../assets/2025q1-alphabet-earnings-release.pdf\"\n",
    "    pdf_file = client.files.upload(file=file_path)\n",
    "    # Ask the model to summarize the earnings release\n",
    "    instruction = \"Summarize the earnings release for the first quarter of 2025\"\n",
    "    response_1 = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=[pdf_file, instruction],\n",
    "    )\n",
    "    # 2nd request which uses the cached prefix (pdf file)\n",
    "    instruction = \"What are focus areas for the second quarter of 2025?\"\n",
    "    response_2 = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=[pdf_file, instruction],\n",
    "    )\n",
    "    if response_2.usage_metadata.cached_content_token_count:\n",
    "        print(\"cached\")\n",
    "        cache_count += 1\n",
    "    else:\n",
    "        print(\"no cached\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
