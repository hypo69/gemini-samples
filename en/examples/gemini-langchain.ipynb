{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce940766",
   "metadata": {},
   "source": [
    "# Google Gemini Cheat Sheet: LangChain\n",
    "\n",
    "The `langchain-google-genai` provides access to Google's powerful Gemini models directly via the Gemini API & Google AI Studio. Google AI Studio enables rapid prototyping and experimentation, making it an ideal starting point for individual developers.\n",
    "\n",
    "[LangChain](https://python.langchain.com/) is a framework for developing AI applications. The `langchain-google-genai` package connects LangChain with Google's Gemini models. [LangGraph](https://python.langchain.com/docs/langgraph/) is a library for building stateful, multi-actor applications with LLMs. \n",
    "\n",
    "All examples use the `gemini-2.0-flash` model. Gemini 2.5 Pro and 2.5 Flash can be used via  `gemini-2.5-pro-preview-03-25` and `gemini-2.5-flash-preview-04-17`. All model ids can be found in the [Gemini API docs](https://ai.google.dev/gemini-api/docs/models).\n",
    "\n",
    "Start for free and get your API key from [Google AI Studio](https://aistudio.google.com/app/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb672c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e58e8",
   "metadata": {},
   "source": [
    "1. Install the package `langchain-google-genai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc43067",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3575e80",
   "metadata": {},
   "source": [
    "## Google Gemini with LangChain Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb11c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'adore la programmation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Simple invocation\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant that translates English to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)  # Output: J'adore la programmation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5a38c",
   "metadata": {},
   "source": [
    "### Chain calls with Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174927d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich liebe Programmieren.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "result = chain.invoke({\n",
    "    \"input_language\": \"English\",\n",
    "    \"output_language\": \"German\",\n",
    "    \"input\": \"I love programming.\",\n",
    "})\n",
    "print(result.content)  # Output: Ich liebe Programmieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68aecf",
   "metadata": {},
   "source": [
    "### Image Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f08da41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a description of the image:\n",
      "\n",
      "**Overall Impression:**\n",
      "\n",
      "The image depicts a serene and majestic mountain landscape, likely at either dawn or dusk, judging by the soft, pastel-colored sky.\n",
      "\n",
      "**Key Elements:**\n",
      "\n",
      "*   **Mountains:** A snow-covered mountain range dominates the scene. One prominent, craggy peak is visible on the right side of the frame. The lower slopes appear to be covered in deep snow.\n",
      "*   **Sky:** The sky is a major feature of the image, filled with layers of clouds. The colors are soft and muted, with hues of pink, orange, and light blue/purple, suggesting the light of sunrise or sunset.\n",
      "*   **Lighting:** The lighting is soft and diffused, creating a tranquil atmosphere. The light seems to be catching the peak of the mountain, highlighting its texture.\n",
      "*   **Foreground:** In the foreground, there's a dark, smooth, snow-covered slope. This creates a sense of depth and leads the eye towards the mountains in the distance.\n",
      "\n",
      "**Atmosphere:**\n",
      "\n",
      "The image evokes a sense of peace, solitude, and the grandeur of nature. The muted colors and soft light contribute to a calming mood.\n",
      "The image is a flow chart, drawn in a simple, hand-drawn style. It begins with a rectangle labeled \"Query.\" An arrow points downward from the \"Query\" box to a diamond shape labeled \"Thought.\" From the \"Thought\" diamond, two paths diverge. One path leads to \"Action\" and \"Observation\" leading to a box labelled \"Tool\". There is also a dashed arrow from the \"Thought\" diamond to a rectangle labeled \"Answer.\" This flow chart represents a process of moving from a query to a thought, which can lead to an action and observation using a tool, or directly to an answer.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    "\n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "# Using an image URL\n",
    "message_url = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"},\n",
    "    ]\n",
    ")\n",
    "result_url = llm.invoke([message_url])\n",
    "print(result_url.content)\n",
    "\n",
    "# Using a local image\n",
    "local_image_path = \"../assets/react.png\"\n",
    "with open(local_image_path, \"rb\") as image_file:\n",
    "    encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "message_local = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{encoded_image}\"}\n",
    "    ]\n",
    ")\n",
    "result_local = llm.invoke([message_local])\n",
    "print(result_local.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742fed04",
   "metadata": {},
   "source": [
    "### Audio Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "392b9adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the Porsche Macan has proven anything, it's that the days of sacrificing performance for practicality are gone, long gone. Engineered to deliver a driving experience like no other, the Macan has demonstrated excellence in style and performance to become the leading sports car in its class. So don't let those five doors fool you. Once you're in the driver's seat, one thing will become immediately clear. This is a Porsche, the Macan, now leasing from 3.99%. Conditions apply.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    "\n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "audio_file_path = \"../assets/porsche.mp3\"\n",
    "audio_mime_type = \"audio/mpeg\"\n",
    "\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Transcribe this audio.\"},\n",
    "        {\"type\": \"media\", \"data\": encoded_audio, \"mime_type\": audio_mime_type}\n",
    "    ]\n",
    ")\n",
    "response = llm.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55ccba8",
   "metadata": {},
   "source": [
    "### Video Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15e9f076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a breakdown of what's happening in the video:\n",
      "\n",
      "*   **0:00-0:09:** The video starts with a gray bar at the bottom and a white space above. A Google search page then appears, with the search query \"What is Gemini 2.5 Flash? When was it launched and what are its key capabilities\" already entered.\n",
      "*   **0:09-0:14:** A cookie consent pop-up appears on the Google search results page, written in German.\n",
      "*   **0:14-0:20:** The cookie consent pop-up remains visible on the Google search results page.\n",
      "*   **0:20-0:34:** The cookie consent pop-up disappears, and the Google search results page is shown.\n",
      "*   **0:34-0:40:** The first search result, \"Start building with Gemini 2.5 Flash\" from Google Blog, is clicked, leading to the corresponding webpage.\n",
      "*   **0:40-0:50:** The \"Start building with Gemini 2.5 Flash\" webpage is displayed.\n",
      "*   **0:50-1:05:** The \"Start building with Gemini 2.5 Flash\" webpage remains visible.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    "\n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "video_file_path = \"../assets/screen.mp4\"\n",
    "video_mime_type = \"video/mp4\"\n",
    "\n",
    "with open(video_file_path, \"rb\") as video_file:\n",
    "    encoded_video = base64.b64encode(video_file.read()).decode('utf-8')\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Describe what's happening in this video.\"},\n",
    "        {\"type\": \"media\", \"data\": encoded_video, \"mime_type\": video_mime_type}\n",
    "    ]\n",
    ")\n",
    "response = llm.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9980d1",
   "metadata": {},
   "source": [
    "### Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e23d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Initialize model for image generation\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash-exp-image-generation\")\n",
    "\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Generate an image of a cat wearing a hat.\",\n",
    "}\n",
    "\n",
    "response = llm.invoke(\n",
    "    [message],\n",
    "    generation_config=dict(response_modalities=[\"TEXT\", \"IMAGE\"]),\n",
    ")\n",
    "\n",
    "# Display the generated image\n",
    "image_base64 = response.content[0].get(\"image_url\").get(\"url\").split(\",\")[-1]\n",
    "image_data = base64.b64decode(image_base64)\n",
    "display(Image(data=image_data, width=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa9fc13",
   "metadata": {},
   "source": [
    "### Tool Calling/Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "925c696f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': '0a7c27f0-b905-4e5f-b941-6fbb04ac4a39', 'type': 'tool_call'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f1/3vdgcm01195b80qcp3t14n_m01b2f5/T/ipykernel_52934/2911867672.py:23: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  content=get_weather(*ai_msg.tool_calls[0]['args']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. It's sunny in San Francisco.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "# Define a tool\n",
    "@tool(description=\"Get the current weather in a given location\")\n",
    "def get_weather(location: str) -> str:\n",
    "    return \"It's sunny.\"\n",
    "\n",
    "# Initialize model and bind the tool\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "llm_with_tools = llm.bind_tools([get_weather])\n",
    "\n",
    "# Invoke with a query that should trigger the tool\n",
    "query = \"What's the weather in San Francisco?\"\n",
    "ai_msg = llm_with_tools.invoke(query)\n",
    "\n",
    "# Access tool calls in the response\n",
    "print(ai_msg.tool_calls)\n",
    "\n",
    "# Pass tool results back to the model\n",
    "tool_message = ToolMessage(\n",
    "    content=get_weather(*ai_msg.tool_calls[0]['args']), \n",
    "    tool_call_id=ai_msg.tool_calls[0]['id']\n",
    ")\n",
    "final_response = llm_with_tools.invoke([ai_msg, tool_message])\n",
    "print(final_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4bd02",
   "metadata": {},
   "source": [
    "### Built-in Tools (Google Search, Code Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48d98121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next total solar eclipse that will be visible from the contiguous United States will be on August 23, 2044. The eclipse will begin in Greenland and pass through Canada before reaching the U.S., ending around sunset in Montana, North Dakota, and South Dakota.\n",
      "\n",
      "Another total solar eclipse will occur on August 12, 2045, and its path will traverse the US coast to coast.\n",
      "Executable code: print(2 * 2)\n",
      "\n",
      "Code execution result: 4\n",
      "\n",
      "2 * 2 = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philschmid/projects/personal/gemini-samples/.venv/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:580: UserWarning: \n",
      "        ⚠️ Warning: Output may vary each run.  \n",
      "        - 'executable_code': Always present.  \n",
      "        - 'execution_result' & 'image_url': May be absent for some queries.  \n",
      "\n",
      "        Validate before using in production.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google.ai.generativelanguage_v1beta.types import Tool as GenAITool\n",
    "\n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "# Google Search\n",
    "search_resp = llm.invoke(\n",
    "    \"When is the next total solar eclipse in US?\",\n",
    "    tools=[GenAITool(google_search={})],\n",
    ")\n",
    "print(search_resp.content)\n",
    "\n",
    "# Code Execution\n",
    "code_resp = llm.invoke(\n",
    "    \"What is 2*2, use python\",\n",
    "    tools=[GenAITool(code_execution={})],\n",
    ")\n",
    "\n",
    "for c in code_resp.content:\n",
    "    if isinstance(c, dict):\n",
    "        if c[\"type\"] == 'code_execution_result':\n",
    "            print(f\"Code execution result: {c['code_execution_result']}\")\n",
    "        elif c[\"type\"] == 'executable_code':\n",
    "            print(f\"Executable code: {c['executable_code']}\")\n",
    "    else:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a6a88e",
   "metadata": {},
   "source": [
    "### Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16343f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philschmid/projects/personal/gemini-samples/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3667: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Abraham Lincoln' height_m=1.93\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Define the desired structure\n",
    "class Person(BaseModel):\n",
    "    '''Information about a person.'''\n",
    "    name: str = Field(..., description=\"The person's name\")\n",
    "    height_m: float = Field(..., description=\"The person's height in meters\")\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(Person)\n",
    "\n",
    "# Invoke the model with a query asking for structured information\n",
    "result = structured_llm.invoke(\"Who was the 16th president of the USA, and how tall was he in meters?\")\n",
    "print(result)  # Output: name='Abraham Lincoln' height_m=1.93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7799f9",
   "metadata": {},
   "source": [
    "### Token Usage Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c8fd7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the art of crafting effective instructions for AI models to elicit desired and accurate outputs.\n",
      "\n",
      "Usage Metadata:\n",
      "{'input_tokens': 10, 'output_tokens': 20, 'total_tokens': 30, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "result = llm.invoke(\"Explain the concept of prompt engineering in one sentence.\")\n",
    "\n",
    "print(result.content)\n",
    "print(\"\\nUsage Metadata:\")\n",
    "print(result.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451854e",
   "metadata": {},
   "source": [
    "## Google Gemini Embeddings with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8848a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-exp-03-07\")\n",
    "\n",
    "# Embed a single query\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "\n",
    "# Embed multiple documents\n",
    "vectors = embeddings.embed_documents([\n",
    "    \"Today is Monday\",\n",
    "    \"Today is Tuesday\",\n",
    "    \"Today is April Fools day\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a2122",
   "metadata": {},
   "source": [
    "### Using with Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bca27ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is the framework for building context-aware reasoning applications\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-exp-03-07\")\n",
    "\n",
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "\n",
    "# Create vector store and retriever\n",
    "vectorstore = InMemoryVectorStore.from_texts([text], embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Retrieve similar documents\n",
    "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
    "print(retrieved_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca406080",
   "metadata": {},
   "source": [
    "### Task Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b21511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 similarity: 0.7892893360164779\n",
      "Document 2 similarity: 0.5410037458373438\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Different task types for different use cases\n",
    "query_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-exp-03-07\", \n",
    "    task_type=\"RETRIEVAL_QUERY\"  # For queries\n",
    ")\n",
    "doc_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-exp-03-07\", \n",
    "    task_type=\"RETRIEVAL_DOCUMENT\"  # For documents\n",
    ")\n",
    "\n",
    "# Compare similarity\n",
    "q_embed = query_embeddings.embed_query(\"What is the capital of France?\")\n",
    "d_embed = doc_embeddings.embed_documents([\"The capital of France is Paris.\", \"Philipp likes to eat pizza.\"])\n",
    "\n",
    "for i, d in enumerate(d_embed):\n",
    "    similarity = cosine_similarity([q_embed], [d])[0][0]\n",
    "    print(f\"Document {i+1} similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d4fa58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
