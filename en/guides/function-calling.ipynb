{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling with Google DeepMind Gemini 2.0 Flash\n",
    "\n",
    "Function calling is the capability to connect LLMs to external tools and to interact with your code and APIs in a structured way. Instead of generating text responses, LLMs understand when to call specific functions and provide the necessary parameters to execute real-world actions.\n",
    "\n",
    "Throughout this guide, we'll look at a practical weather-based assistant access to a weather API. Yes, not very creative, but there is a free API we can use and it should be enough to demonstrate the concept understand how you can use function calling to build a more complex assistant.  \n",
    "\n",
    "This guide covers: \n",
    "\n",
    "1. [How does function calling work?](#how-does-function-calling-work)\n",
    "2. [When to use function calling?](#when-to-use-function-calling)\n",
    "3. [Function Calling with Google Gemini 2.0 Flash](#function-calling-with-google-gemini-20-flash)\n",
    "4. [Advanced: Function Calling with LangChain](#advanced-function-calling-with-langchain)\n",
    "5. [Advanced: Function Calling with OpenAI Compatible API](#advanced-function-calling-with-openai-compatible-api)\n",
    "\n",
    "## How does function calling work?\n",
    "\n",
    "Function calling may imply that the LLM is directly performing some action. This is not the case! When a user prompts an LLM with function calling, the model analyzes the input and determines if and which function would be the most appropriate for the task (can be a single function or multiple functions). Instead of providing a text response, the model generates a structured JSON object that specifies which function to call and the necessary parameters. \n",
    "\n",
    "![Function Intro](../assets/function-intro.png)\n",
    "\n",
    "In practice function calling not only describe the process of generating structured output, but also the process of calling a function and how to handle the output. As you don't want to return the raw output of the function to your user, you want the LLM to generate an appropriate response, based on the conversation history.\n",
    "\n",
    "![Function calling](../assets/function-calling.png)\n",
    "\n",
    "\n",
    "Practical Function Calling follows these steps:\n",
    "1. Your application sends a prompt to the LLM along with function definitions\n",
    "2. The LLM analyzes the prompt and decides whether to respond directly or use defined functions\n",
    "3. If using functions, the LLM generates structured arguments for the function call\n",
    "4. Your application receives the function call details and executes the actual function\n",
    "5. The function results are sent back to the LLM\n",
    "6. The LLM provides a final response incorporating the function results\n",
    "\n",
    "This cycle can continue as needed, allowing for complex multi-step interactions between the application and the LLM. It is also possible that the LLM decides that it needs to call multiple functions after each other or in parallel before returning a final response to the user.\n",
    "\n",
    "\n",
    "## When to Use Function Calling?\n",
    "\n",
    "Function calling has emerged as one of the popular methods for building AI agents. It can help build human-AI interfaces that access and query real-time information from external sources like APIs, databases, and knowledge bases while providing a natural language interface (text or audio) to users.\n",
    "\n",
    "Function calling enables automation tasks like scheduling appointments, creating invoices, or sending reminders. An example usecase could be a customer service assistant might use function calling to seamlessly handle tasks like checking order status, processing returns, and updating customer information â€“ all while maintaining a natural conversation flow with the user.\n",
    "\n",
    "You now longer need to build Applications which required complex forms or multiple steps to collect information from the user. Instead, you can build a natural language interface that allows the user to interact with the application in a conversational way. Or have no user interface at all and let the LLM interact with the world on your behalf.\n",
    "\n",
    "## Function Calling with Google Gemini 2.0 Flash\n",
    "\n",
    "Google Gemini 2.0 Flash supports function calling through multiple interfaces, [OpenAPI compatible JSON Schema](https://spec.openapis.org/oas/v3.0.3#schema) and Python functions defintions with docstrings. If you are using JavaScript/Typescript you currently have to use the JSON Schema interface. The Python SDK `google-genai` can automatically generate the JSON Schema from the Python function definitions and docstrings. We are going to take a look at both interfaces. \n",
    "\n",
    "_Note: Gemini 2.0 Flash currently doesn't support `anyOf` type in the JSON Schema._\n",
    "\n",
    "Lets start with the JSON Schema interface, but before that lets install the `google-genai` library and make sure we have a Gemini API key. If you don't have one yet you can get one from [Google AI Studio](https://aistudio.google.com/app/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"google-genai>=1.0.0\" geopy requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the SDK and API key, you can create a client and define the model you are going to use the new Gemini 2.0 Flash model, which is available via free tier with 1,500 request per day (at 2025-02-06)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "# create client\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "# Define the model you are going to use\n",
    "model_id =  \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, lets quickly test if we have access to the model and can generate some text.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuremberg is home to the oldest Christmas market in Germany, the Christkindlesmarkt, which dates back to the mid-16th century.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=[\"Tell me 1 good fact about Nuremberg.\"]\n",
    ")\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Calling with JSON Schema\n",
    "\n",
    "For using Function Calling with JSON Schema we need to define our functions as JSON Schema. Let's create a simple weather function as an example. The main parts of the JSON Schema are:\n",
    "\n",
    "- `name`: name of the function, this need to match the name of your function in your code \n",
    "- `description`: description of what the function does. This is important as this information will be used by the LLM to identify when to use the function\n",
    "- `parameters`: JSON schema object of type definition for the input arguments of your function. Each parameter has a type, e.g. `string` and a `description` which are used by the LLM what to add here.\n",
    "- `required`: What `parameters` are required if not all required the LLM might not provide an argument when it thinks its not needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_function = {\n",
    "    \"name\": \"get_weather_forecast\",\n",
    "    \"description\": \"Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
    "            },\n",
    "            \"date\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"the forecasting date for when to get the weather format (yyyy-mm-dd)\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\",\"date\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this function definition and add it to our LLM call. The LLM will then decide on its own if it should \"call\" the function or return a normal text response. Lets test this. Function declarations are defined in the `config` object. We use the Pydantic `GenerateContentConfig` data structure to define the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai.types import GenerateContentConfig\n",
    "\n",
    "# Generation Config\n",
    "config = GenerateContentConfig(\n",
    "    system_instruction=\"You are a helpful assistant that use tools to access and retrieve information from a weather API. Today is 2025-03-04.\", # to give the LLM context on the current date.\n",
    "    tools=[{\"function_declarations\": [weather_function]}], # define the functions that the LLM can use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets try without our tool using \"Whats the weather in Berlin this today?\" prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't give you a real-time weather update for Berlin. To get the most accurate and current weather information, I recommend checking a reliable weather source like:\n",
      "\n",
      "*   **A weather app:** (e.g., WeatherBug, AccuWeather, The Weather Channel)\n",
      "*   **A weather website:** (e.g., Google Weather, [weather.com](http://weather.com))\n",
      "*   **A local news source:** (e.g., a Berlin news website or TV station)\n",
      "\n",
      "These sources will provide you with up-to-the-minute details on temperature, wind, precipitation, and more.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents='Whats the weather in Berlin this today?'\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the output is not helpful, as the LLM does not know how to answer the question. Now lets try with our function.\n",
    "\n",
    "_Note: When the LLM decides to use a tool the `.text` attribute might be null as the function call is returned in the `function_call` attribute of each candidate._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=None args={'date': '2025-03-04', 'location': 'Berlin, DE'} name='get_weather_forecast'\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    config=config,\n",
    "    contents='Whats the weather in Berlin today?'\n",
    ")\n",
    "\n",
    "# iterate over eacht return part and check if it is a function call or a normal response\n",
    "for part in response.candidates[0].content.parts:\n",
    "    print(part.function_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, Gemini correctly identified that it needs to call our function and generated the structured response including the function name and arguments. Now, lets put this into a \"agentic\" method that will call the Gemini then check if the response is a function call and if so call the function with the arguments and finally generate a final response.\n",
    "\n",
    "_Note: The code below uses the available `types` data structured from the `google-genai` library to create the conversation history._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call detected\n",
      "Calling tool: get_weather_forecast with args: {'date': '2025-03-04', 'location': 'Berlin, DE'}\n",
      "Calling LLM with tool results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OK. Today in Berlin, the temperature will be between 1.7 and 12.2 degrees Celsius.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.genai import types\n",
    "from geopy.geocoders import Nominatim\n",
    "import requests\n",
    "\n",
    "# Simple function to get the weather forecast for a given location and date\n",
    "geolocator = Nominatim(user_agent=\"weather-app\") \n",
    "def get_weather_forecast(location, date):\n",
    "    location = geolocator.geocode(location)\n",
    "    if location:\n",
    "        try:\n",
    "            response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}\")\n",
    "            data = response.json()\n",
    "            return {time: temp for time, temp in zip(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    else:\n",
    "        return {\"error\": \"Location not found\"}\n",
    "\n",
    "# Function dictionary to map the function name to the function\n",
    "functions = {\n",
    "    \"get_weather_forecast\": get_weather_forecast\n",
    "}\n",
    "\n",
    "# helper function to call the function\n",
    "def call_function(function_name, **kwargs):\n",
    "    return functions[function_name](**kwargs)\n",
    "\n",
    "# agentic loop to handle the function call\n",
    "def function_call_loop(prompt):\n",
    "    # create the conversation\n",
    "    contents = [types.Content(role=\"user\", parts=[types.Part(text=prompt)])]\n",
    "    # initial request \n",
    "    response = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        config=config,\n",
    "        contents=contents\n",
    "    )\n",
    "    for part in response.candidates[0].content.parts:\n",
    "        # add response to the conversation\n",
    "        contents.append(types.Content(role=\"model\", parts=[part]))\n",
    "        # check if the response is a function call\n",
    "        if part.function_call:\n",
    "            print(\"Tool call detected\")\n",
    "            function_call = part.function_call\n",
    "            # Call the tool with arguments\n",
    "            print(f\"Calling tool: {function_call.name} with args: {function_call.args}\")\n",
    "            tool_result = call_function(function_call.name, **function_call.args)\n",
    "            # Build the response parts using the function result.\n",
    "            function_response_part = types.Part.from_function_response(\n",
    "                name=function_call.name,\n",
    "                response={\"result\": tool_result},\n",
    "            )\n",
    "            contents.append(types.Content(role=\"user\", parts=[function_response_part]))\n",
    "            # Send follow-up with tool results, but remove the tools from the config\n",
    "            print(f\"Calling LLM with tool results\")\n",
    "            func_gen_response = client.models.generate_content(\n",
    "                model=model_id, config=config, contents=contents\n",
    "            )\n",
    "            # Add the reponse to the conversation\n",
    "            contents.append(types.Content(role=\"model\", parts=[func_gen_response]))\n",
    "    # return the final response\n",
    "    return contents[-1].parts[0].text.strip()\n",
    "    \n",
    "\n",
    "function_call_loop(\"Whats the weather in Berlin today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We successfully called our function and generated a final response using the function result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Calling using Python functions\n",
    "\n",
    "The Python SDK `google-genai` can automatically generate the JSON Schema from the Python function definitions and docstrings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import requests\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"weather-app\") \n",
    "\n",
    "def get_weather_forecast(location: str, date: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\"\n",
    "    \n",
    "    Args:\n",
    "        location (str): The city and state, e.g., San Francisco, CA\n",
    "        date (str): The forecasting date for when to get the weather format (yyyy-mm-dd)\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary with the time as key and the temperature as value\n",
    "    \"\"\"\n",
    "    location = geolocator.geocode(location)\n",
    "    if location:\n",
    "        try:\n",
    "            response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}\")\n",
    "            data = response.json()\n",
    "            return {time: temp for time, temp in zip(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    else:\n",
    "        return {\"error\": \"Location not found\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the JSON Schema example we add our function to the generation config and we disable the automatic function calling for now, more on that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai.types import GenerateContentConfig\n",
    "\n",
    "# Generation Config\n",
    "config = GenerateContentConfig(\n",
    "    system_instruction=\"You are a helpful assistant that can help with weather related questions. Today is 2025-03-04.\", # to give the LLM context on the current date.\n",
    "    tools=[get_weather_forecast], # define the functions that the LLM can use\n",
    "    automatic_function_calling={\"disable\": True} # Disable for now. \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`automatic_function_calling.disable` is set to `True`. And `automatic_function_calling.maximum_remote_calls` is a positive number 10. Disabling automatic function calling. If you want to enable automatic function calling, please set `automatic_function_calling.disable` to `False` or leave it unset, and set `automatic_function_calling.maximum_remote_calls` to a positive integer or leave `automatic_function_calling.maximum_remote_calls` unset.\n",
      "/Users/philschmid/projects/personal/gemini-samples/.venv/lib/python3.11/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `enum` but got `str` with value `'STRING'` - serialized value may not be as expected\n",
      "  Expected `enum` but got `str` with value `'STRING'` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=None args={'location': 'Berlin, Germany', 'date': '2025-03-04'} name='get_weather_forecast'\n"
     ]
    }
   ],
   "source": [
    "r = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    config=config,\n",
    "    contents='Whats the weather in Berlin today?'\n",
    ")\n",
    "# iterate over eacht return part and check if it is a function call or a normal response\n",
    "for part in r.candidates[0].content.parts:\n",
    "    print(part.function_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Similar to our JSON Schema example Gemini correctly identified that it needs to call our function. The next step would be to implement the same logic to identify the function to call and handle the output, but the Python SDK supports this out of the box. \n",
    "\n",
    "If we enable the `automatic_function_calling` the SDK will automatically call the function, and sends another request to Gemini with the function result. We can remove the `automatic_function_calling` as the default behavior when Python functions are used as tools is to automatically call the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. Today in Berlin, the temperature will be between 1.7 and 12.2 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "from google.genai.types import GenerateContentConfig\n",
    "\n",
    "# Generation Config\n",
    "config = GenerateContentConfig(\n",
    "    system_instruction=\"You are a helpful assistant that use tools to access and retrieve information from a weather API. Today is 2025-03-04.\", # to give the LLM context on the current date.\n",
    "    tools=[get_weather_forecast], # define the functions that the LLM can use\n",
    "    # removed the automatic_function_calling as the default is to call the function\n",
    ")\n",
    "\n",
    "r = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    config=config,\n",
    "    contents='Whats the weather in Berlin today?'\n",
    ")\n",
    "\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now, lets try an example which might be closer to a real usecase, where we provide more context to our Assistant about the user to have a more natural conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The temperature in Nuremberg will range from 0.6 degrees Celsius to 13.2 degrees Celsius today. I would recommend bringing a jacket.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.genai.types import GenerateContentConfig\n",
    "\n",
    "# Generation Config\n",
    "config = GenerateContentConfig(\n",
    "    system_instruction=\"You are a helpful assistant that use tools to access and retrieve information from a weather API.\",\n",
    "    tools=[get_weather_forecast], # define the functions that the LLM can use\n",
    "    # removed the automatic_function_calling as the default with callable functions is to call the function\n",
    ")\n",
    "\n",
    "# Prompt includes more context about the user and the current date\n",
    "prompt = f\"\"\"\n",
    "Today is 2025-03-04. You are chatting with Philipp, you have access to more information about him.\n",
    "\n",
    "User Context:\n",
    "- name: Philipp\n",
    "- location: Nuremberg\n",
    "\n",
    "User: Can i wear a T-shirt later today?\"\"\"\n",
    "\n",
    "r = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    config=config,\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Advanced: Function Calling with LangChain \n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/introduction/) is a composable framework that simplifies the development of LLM-powered application. LangChain supports Google Gemini 2.0 Flash and the function calling capabilities. [LangGraph](https://langchain-ai.github.io/langgraph/) is an orchestration framework for controllable agentic workflows, and many companies use LangChain and LangGraph together to build AI Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Gemini with LangChain we need to create a `ChatGoogleGenerativeAI` class, that implements the `BaseChatModel` interface, which is responsible for the LLM calls and supporting function calling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have access to real-time information, including live weather updates. To find out the weather in Berlin today, I recommend checking a reliable weather app or website such as:\n",
      "\n",
      "*   **Google Weather:** Just search \"weather in Berlin\" on Google.\n",
      "*   **AccuWeather:** [https://www.accuweather.com/](https://www.accuweather.com/)\n",
      "*   **The Weather Channel:** [https://weather.com/](https://weather.com/)\n",
      "*   **Local German weather services:** such as Deutscher Wetterdienst (DWD)\n",
      "\n",
      "These sources will provide you with the most up-to-date and accurate weather information for Berlin.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Get API key and define model id\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\n",
    "model_id =  \"gemini-2.0-flash\"\n",
    "\n",
    "# Create LLM class \n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=model_id,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    google_api_key=api_key,\n",
    ")\n",
    "\n",
    "# lets try it\n",
    "res = llm.invoke(\"What is the weather in Berlin today?\")\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! This looks similar to our initial call without tools enabled. Now lets try to add the function calling capabilities. Similar to the [SDK LangChain supports automatic python function](https://python.langchain.com/docs/concepts/tool_calling/) to tool conversion. If you want to use a function as tool you can add a `@tool` decorator to the function. \n",
    "\n",
    "_Note: We copy the code from out `get_weather_forecast` function from the Python SDK example._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import requests\n",
    "from langchain.tools import tool\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"weather-app\") \n",
    "\n",
    "@tool\n",
    "def get_weather_forecast(location: str, date: str) -> str:\n",
    "    \"\"\"Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\"\n",
    "    \n",
    "    Args:\n",
    "        location (str): The city and state, e.g., San Francisco, CA\n",
    "        date (str): The forecasting date for when to get the weather format (yyyy-mm-dd)\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary with the time as key and the temperature as value\n",
    "    \"\"\"\n",
    "    location = geolocator.geocode(location)\n",
    "    if location:\n",
    "        try:\n",
    "            response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}\")\n",
    "            data = response.json()\n",
    "            return {time: temp for time, temp in zip(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    else:\n",
    "        return {\"error\": \"Location not found\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have our tool defined we can `bind` it to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([get_weather_forecast])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'get_weather_forecast', 'args': {'date': '2025-03-04', 'location': 'Berlin, DE'}, 'id': 'c0043a1b-4430-4f7a-a0d6-35bd4ffc6501', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that use tools to access and retrieve information from a weather API. Today is 2025-03-04.\",\n",
    "    ),\n",
    "    (\"human\", \"What is the weather in Berlin today?\"),\n",
    "]\n",
    "\n",
    "# Call the LLM with the messages and tools\n",
    "res = llm_with_tools.invoke(messages)\n",
    "\n",
    "# Check if the LLM returned a function call\n",
    "if res.tool_calls:\n",
    "    print(res.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It worked. Now, we would need to call our function with the arguments again and add the result to the conversation. Similar to the Python SDK example Langchain supports automatic function calling, through the `create_tool_calling_agent` and `AgentExecutor`. \n",
    "\n",
    "- `create_tool_calling_agent`: Creates an agent that can:\n",
    "  - Understand when to use available tools based on user input\n",
    "  - Generate structured arguments for tool calls\n",
    "  - Process tool outputs to create natural responses\n",
    "\n",
    "- `AgentExecutor`: Handles the execution flow by:\n",
    "  - Managing the conversation between user and agent\n",
    "  - Automatically calling tools when the agent requests them\n",
    "  - Handling any errors during tool execution\n",
    "  - Maintaining conversation context across multiple interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_weather_forecast` with `{'date': '2025-03-04', 'location': 'Berlin, DE'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'2025-03-04T00:00': 3.5, '2025-03-04T01:00': 3.4, '2025-03-04T02:00': 3.2, '2025-03-04T03:00': 2.4, '2025-03-04T04:00': 2.4, '2025-03-04T05:00': 2.1, '2025-03-04T06:00': 1.7, '2025-03-04T07:00': 1.9, '2025-03-04T08:00': 3.3, '2025-03-04T09:00': 5.2, '2025-03-04T10:00': 6.9, '2025-03-04T11:00': 8.5, '2025-03-04T12:00': 10.5, '2025-03-04T13:00': 11.4, '2025-03-04T14:00': 11.8, '2025-03-04T15:00': 12.2, '2025-03-04T16:00': 11.6, '2025-03-04T17:00': 10.6, '2025-03-04T18:00': 9.6, '2025-03-04T19:00': 8.6, '2025-03-04T20:00': 7.8, '2025-03-04T21:00': 6.9, '2025-03-04T22:00': 6.3, '2025-03-04T23:00': 5.8}\u001b[0m\u001b[32;1m\u001b[1;3mOK. Today in Berlin, the temperature will be between 1.7 and 12.2 degrees Celsius.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Initialize the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that use tools to access and retrieve information from a weather API. Today is 2025-03-04.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    \n",
    "])\n",
    "\n",
    "# Create the agent and executor with out llm, tools and prompt\n",
    "agent = create_tool_calling_agent(llm_with_tools, [get_weather_forecast],prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=[get_weather_forecast], verbose=True)\n",
    "\n",
    "# Run our query \n",
    "res = agent_executor.invoke({\"input\": \"What is the weather in Berlin today?\"})\n",
    "print(res[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! It worked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Function Calling with OpenAI Compatible API\n",
    "\n",
    "Google Gemini has an [OpenAI compatible API](https://ai.google.dev/gemini-api/docs/openai), which allows us to use Gemini models with the OpenAI API and SDKs. The API supports function calling out of the box, meaning we can use the OpenAI features to call our function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Get API key and define model id\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\n",
    "model_id =  \"gemini-2.0-flash\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try it out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have real-time access to live weather data. To find out the weather in Berlin today, I recommend checking a reliable weather source such as:\n",
      "\n",
      "*   **A weather app:** (e.g., WeatherBug, AccuWeather, The Weather Channel)\n",
      "*   **A weather website:** (e.g., Google Weather, a local news site)\n",
      "\n",
      "These sources will give you the most up-to-date and accurate information.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=[{\"role\": \"user\", \"content\": \"What is the weather in Berlin today?\"}],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now lets our JSON Schema example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(arguments='{\"date\":\"2025-03-04\",\"location\":\"Berlin, DE\"}', name='get_weather_forecast')\n"
     ]
    }
   ],
   "source": [
    "weather_function =   {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "    \"name\": \"get_weather_forecast\",\n",
    "    \"description\": \"Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
    "            },\n",
    "            \"date\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"the forecasting date for when to get the weather format (yyyy-mm-dd)\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\",\"date\"]\n",
    "    }\n",
    "}}\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant that use tools to access and retrieve information from a weather API. Today is 2025-03-04.\"},\n",
    "      {\"role\": \"user\", \"content\": \"What is the weather in Berlin today?\"}],\n",
    "  tools=[weather_function],\n",
    "  tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "if response.choices[0].message.tool_calls:\n",
    "    print(response.choices[0].message.tool_calls[0].function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We successfully called our function and generated the structured response. If you are using the OpenAI SDK you can now easily test Gemini function calling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
